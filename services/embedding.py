"""
Image Embedding service - CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ë²¡í„°í™”
512ì°¨ì› ì„ë² ë”© ìƒì„± ë° ë²¡í„° ê²€ìƒ‰
"""

import os
from typing import List, Optional, Tuple
from io import BytesIO
from PIL import Image
import numpy as np

# CLIP ëª¨ë¸
try:
    from transformers import CLIPProcessor, CLIPModel
    import torch
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("[Embedding] âš ï¸ transformers or torch not installed. CLIP will be unavailable.")

# ì „ì—­ ëª¨ë¸ ìºì‹±
_clip_model = None
_clip_processor = None
_device = None


def load_clip_model() -> Tuple[Optional[object], Optional[object], Optional[str]]:
    """
    CLIP ëª¨ë¸ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´)
    
    Returns:
        (model, processor, device)
    """
    global _clip_model, _clip_processor, _device
    
    if not CLIP_AVAILABLE:
        print("[Embedding] âŒ CLIP not available")
        return None, None, None
    
    # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ì¬ì‚¬ìš©
    if _clip_model is not None:
        return _clip_model, _clip_processor, _device
    
    try:
        print("[Embedding] ğŸ”„ Loading CLIP model...")
        
        # ë””ë°”ì´ìŠ¤ ì„ íƒ (GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU)
        _device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"[Embedding] ğŸ–¥ï¸ Using device: {_device}")
        
        # CLIP ëª¨ë¸ ë¡œë“œ (openai/clip-vit-base-patch32)
        model_name = "openai/clip-vit-base-patch32"
        _clip_model = CLIPModel.from_pretrained(model_name).to(_device)
        _clip_processor = CLIPProcessor.from_pretrained(model_name)
        
        # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (ì¶”ë¡ ìš©)
        _clip_model.eval()
        
        print("[Embedding] âœ… CLIP model loaded successfully")
        return _clip_model, _clip_processor, _device
    
    except Exception as e:
        print(f"[Embedding] âŒ Failed to load CLIP model: {e}")
        return None, None, None


def generate_image_embedding(image_bytes: bytes) -> Optional[List[float]]:
    """
    ì´ë¯¸ì§€ë¡œë¶€í„° 512ì°¨ì› ì„ë² ë”© ë²¡í„° ìƒì„±
    
    Args:
        image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
    
    Returns:
        512ì°¨ì› ì„ë² ë”© ë²¡í„° (ë¦¬ìŠ¤íŠ¸)
    """
    model, processor, device = load_clip_model()
    
    if model is None or processor is None:
        print("[Embedding] âŒ CLIP model not available")
        return None
    
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(BytesIO(image_bytes))
        
        # RGB ë³€í™˜
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        inputs = processor(images=image, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ì¶”ë¡  (gradient ê³„ì‚° ë¹„í™œì„±í™”)
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
        
        # ì •ê·œí™” (L2 norm)
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ
        embedding = image_features.cpu().numpy().flatten().tolist()
        
        print(f"[Embedding] âœ… Generated embedding: {len(embedding)} dimensions")
        return embedding
    
    except Exception as e:
        print(f"[Embedding] âŒ Embedding generation failed: {e}")
        return None


def generate_embeddings_batch(
    image_bytes_list: List[bytes],
    batch_size: int = 8
) -> List[Optional[List[float]]]:
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ìƒì„± (íš¨ìœ¨ì )
    
    Args:
        image_bytes_list: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸°
    
    Returns:
        ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    model, processor, device = load_clip_model()
    
    if model is None or processor is None:
        print("[Embedding] âŒ CLIP model not available")
        return [None] * len(image_bytes_list)
    
    embeddings = []
    
    try:
        # ë°°ì¹˜ë¡œ ì²˜ë¦¬
        for i in range(0, len(image_bytes_list), batch_size):
            batch = image_bytes_list[i:i+batch_size]
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
            images = []
            for img_bytes in batch:
                try:
                    img = Image.open(BytesIO(img_bytes))
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    images.append(img)
                except Exception as e:
                    print(f"[Embedding] âš ï¸ Failed to load image: {e}")
                    images.append(None)
            
            # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬
            valid_images = [img for img in images if img is not None]
            
            if not valid_images:
                embeddings.extend([None] * len(batch))
                continue
            
            # ë°°ì¹˜ ì „ì²˜ë¦¬
            inputs = processor(images=valid_images, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ì¶”ë¡ 
            with torch.no_grad():
                image_features = model.get_image_features(**inputs)
            
            # ì •ê·œí™”
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # NumPy ë°°ì—´ë¡œ ë³€í™˜
            batch_embeddings = image_features.cpu().numpy()
            
            # ê²°ê³¼ ë§¤í•‘ (None ì´ë¯¸ì§€ í¬í•¨)
            valid_idx = 0
            for img in images:
                if img is not None:
                    embeddings.append(batch_embeddings[valid_idx].tolist())
                    valid_idx += 1
                else:
                    embeddings.append(None)
            
            print(f"[Embedding] âœ… Batch {i//batch_size + 1}: {len(valid_images)}/{len(batch)} processed")
        
        return embeddings
    
    except Exception as e:
        print(f"[Embedding] âŒ Batch embedding failed: {e}")
        return [None] * len(image_bytes_list)


def generate_text_embedding(text: str) -> Optional[List[float]]:
    """
    í…ìŠ¤íŠ¸ë¡œë¶€í„° ì„ë² ë”© ë²¡í„° ìƒì„± (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰ìš©)
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
    
    Returns:
        512ì°¨ì› ì„ë² ë”© ë²¡í„°
    """
    model, processor, device = load_clip_model()
    
    if model is None or processor is None:
        print("[Embedding] âŒ CLIP model not available")
        return None
    
    try:
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        inputs = processor(text=[text], return_tensors="pt", padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ì¶”ë¡ 
        with torch.no_grad():
            text_features = model.get_text_features(**inputs)
        
        # ì •ê·œí™”
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        embedding = text_features.cpu().numpy().flatten().tolist()
        
        print(f"[Embedding] âœ… Generated text embedding: {len(embedding)} dimensions")
        return embedding
    
    except Exception as e:
        print(f"[Embedding] âŒ Text embedding failed: {e}")
        return None


def calculate_cosine_similarity(
    embedding1: List[float],
    embedding2: List[float]
) -> float:
    """
    ë‘ ì„ë² ë”© ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        embedding1: ì²« ë²ˆì§¸ ë²¡í„°
        embedding2: ë‘ ë²ˆì§¸ ë²¡í„°
    
    Returns:
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0 ~ 1.0)
    """
    try:
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        vec1 = np.array(embedding1)
        vec2 = np.array(embedding2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        similarity = dot_product / (norm1 * norm2)
        
        # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        similarity = (similarity + 1.0) / 2.0
        
        return float(similarity)
    
    except Exception as e:
        print(f"[Embedding] âŒ Similarity calculation failed: {e}")
        return 0.0


def hash_image(image_bytes: bytes) -> str:
    """
    ì´ë¯¸ì§€ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš©)
    
    Args:
        image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
    
    Returns:
        SHA-256 í•´ì‹œ ë¬¸ìì—´
    """
    import hashlib
    return hashlib.sha256(image_bytes).hexdigest()


def preload_model():
    """
    ì„œë²„ ì‹œì‘ ì‹œ CLIP ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (ì²« ìš”ì²­ ì§€ì—° ë°©ì§€)
    """
    print("[Embedding] ğŸš€ Preloading CLIP model...")
    load_clip_model()
    print("[Embedding] âœ… Model preload complete")


# í™˜ê²½ ë³€ìˆ˜ë¡œ ìë™ ë¡œë“œ ì„¤ì • ê°€ëŠ¥
if os.getenv("PRELOAD_CLIP_MODEL", "false").lower() == "true":
    preload_model()
